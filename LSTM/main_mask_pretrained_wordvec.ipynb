{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "torch.manual_seed(5)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# ############################################################################\n",
    "# param\n",
    "##############################################################################\n",
    "batch_size=50\n",
    "use_cuda=1\n",
    "##############################################################################\n",
    "# Load data\n",
    "##############################################################################\n",
    "from data_loader import DataLoader\n",
    "#question\n",
    "path='/media/kwane/3160053c-937e-4de9-a540-b28bd2802040/kwane/NLP/lstm_text_class/data/rt/'\n",
    "pretrained_wordvec=np.load(path+'pretrained_wordvec.npy')\n",
    "data = torch.load(path+'corpus.pt')\n",
    "max_len = data[\"max_len\"]\n",
    "vocab_size = data['dict']['vocab_size']\n",
    "label_size = data['dict']['label_size']\n",
    "\n",
    "training_data = DataLoader(\n",
    "             data['train']['src'],\n",
    "             data['train']['label'],\n",
    "             data['train']['train_mask'],\n",
    "             data['train']['train_mask_all'],\n",
    "             max_len,\n",
    "             batch_size=batch_size)\n",
    "\n",
    "validation_data = DataLoader(\n",
    "              data['valid']['src'],\n",
    "              data['valid']['label'],\n",
    "              data['valid']['valid_mask'],\n",
    "              data['valid']['valid_mask_all'],\n",
    "              max_len,\n",
    "              batch_size=batch_size,\n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named const",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6c454e42cb6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mconst\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named const"
     ]
    }
   ],
   "source": [
    "#model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import const\n",
    "torch.manual_seed(5)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        mu = torch.mean(input, dim=-1, keepdim=True)\n",
    "        sigma = torch.std(input, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "        output = (input - mu) / sigma\n",
    "        return output * self.weight.expand_as(output) + self.bias.expand_as(output)\n",
    "\n",
    "class LSTM_Text(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,batch_size,embed_dim,label_size):\n",
    "        super(LSTM_Text,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.hidden_size=200\n",
    "        self.lstm_layers=1\n",
    "        self.dropout=0.5\n",
    "        self.batch_size=batch_size\n",
    "        self.bidirectional=True\n",
    "        self.label_size=label_size\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        #self.num_directions = 2 if 1 else 1\n",
    "\n",
    "        self.lookup_table = nn.Embedding(self.vocab_size, self.embed_dim,\n",
    "                                padding_idx=const.PAD)\n",
    "        self.lstm = nn.LSTM(self.embed_dim,\n",
    "                            self.hidden_size,\n",
    "                            self.lstm_layers,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "        self.ln = LayerNorm(self.hidden_size*self.num_directions)\n",
    "        self.encode_ln = LayerNorm(self.embed_dim)\n",
    "\n",
    "        self.logistic = nn.Linear(self.hidden_size*self.num_directions,\n",
    "                                self.label_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self, scope=1.):\n",
    "        #self.lookup_table.weight.data.uniform_(-scope, scope)\n",
    "        self.lookup_table.weight.data.copy_(torch.from_numpy(pretrained_wordvec))\n",
    "        self.logistic.weight.data.uniform_(-scope, scope)\n",
    "        self.logistic.bias.data.fill_(0)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        num_layers = self.lstm_layers*self.num_directions\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(num_layers, self.batch_size, self.hidden_size).zero_()),Variable(weight.new(num_layers, self.batch_size, self.hidden_size).zero_()))\n",
    "\n",
    "    def forward(self, input,mask, hidden):\n",
    "        encode = self.lookup_table(input)\n",
    "        encode = self.encode_ln(encode)#batch_norm\n",
    "        lstm_out, hidden = self.lstm(encode.transpose(0, 1), hidden)\n",
    "        \n",
    "        output = F.tanh(lstm_out)\n",
    "        final_h = torch.mul(output, mask[:, :, None].transpose(0, 1).type(torch.cuda.FloatTensor))\n",
    "        final_h=torch.sum(final_h.transpose(0, 1), 1)\n",
    "        \n",
    "        return self.logistic(final_h), hidden\n",
    "# ##############################################################################\n",
    "# Build model\n",
    "# ##############################################################################\n",
    "import model\n",
    "\n",
    "\n",
    "rnn = LSTM_Text(vocab_size=vocab_size,batch_size=batch_size,embed_dim=300,label_size=label_size)\n",
    "if use_cuda:\n",
    "    rnn = rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ##############################################################################\n",
    "# Training\n",
    "# ##############################################################################\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "epoches=100\n",
    "starttime = datetime.datetime.now()\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        if use_cuda:\n",
    "            return Variable(h.data).cuda()\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "    \n",
    "def train():\n",
    "    loss_plt=[]\n",
    "    hidden = rnn.init_hidden()\n",
    "    for step,(data,label,mask,mask_all) in enumerate(training_data):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        target, hidden = rnn(data,mask, hidden)\n",
    "        loss = criterion(target, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        #print step,('loss: %.3f' % ( loss.data[0]))\n",
    "        loss_plt.append(loss.data[0]) \n",
    "    return loss_plt\n",
    "\n",
    "def test():\n",
    "    rnn.eval()\n",
    "    corrects = eval_loss = 0\n",
    "    _size = validation_data.sents_size\n",
    "    hidden = rnn.init_hidden()\n",
    "    for step,(data,label,mask,mask_all) in enumerate(validation_data):\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        pred, hidden = rnn(data,mask, hidden)\n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        eval_loss += loss.data\n",
    "        corrects += (torch.max(pred, 1)[1].view(label.size()).data == label.data).sum()\n",
    "\n",
    "    return 1.0*corrects/_size * 100.0,corrects,_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#main\n",
    "\n",
    "epoches=10\n",
    "\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "loss_plt=[]\n",
    "Accuracy_=[]\n",
    "for epoch in range(epoches):\n",
    "    #train\n",
    "    loss_plt+=train()\n",
    "    \n",
    "    #test\n",
    "    Accuracy,corrects,_size=test()\n",
    "    Accuracy_.append(Accuracy)\n",
    "    print 'epoch:',epoch,'Accuracy:',str(Accuracy)+'%',str(corrects)+'/'+str(_size)#,loss_plt[-1]\n",
    "    \n",
    "plt.plot(loss_plt)  \n",
    "plt.xlabel('num')  \n",
    "plt.ylabel('loss')  \n",
    "plt.show()\n",
    "\n",
    "plt.plot(Accuracy_)  \n",
    "plt.xlabel('epochs')  \n",
    "plt.ylabel('Accuracy')  \n",
    "plt.show()\n",
    "        \n",
    "print('Finished Training')\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "print 'starttime:',starttime,'|endtime:',endtime\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
