{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "torch.manual_seed(5)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# ############################################################################\n",
    "# param\n",
    "##############################################################################\n",
    "batch_size=50\n",
    "use_cuda=1\n",
    "##############################################################################\n",
    "# Load data\n",
    "##############################################################################\n",
    "from data_loader import DataLoader\n",
    "#question\n",
    "path='/media/kwane/3160053c-937e-4de9-a540-b28bd2802040/kwane/NLP/lstm_text_class/data/rt/'\n",
    "pretrained_wordvec=np.load(path+'pretrained_wordvec.npy')\n",
    "data = torch.load(path+'corpus.pt')\n",
    "max_len = data[\"max_len\"]\n",
    "vocab_size = data['dict']['vocab_size']\n",
    "label_size = data['dict']['label_size']\n",
    "\n",
    "training_data = DataLoader(\n",
    "             data['train']['src'],\n",
    "             data['train']['label'],\n",
    "             data['train']['train_mask'],\n",
    "             data['train']['train_mask_all'],\n",
    "             max_len,\n",
    "             batch_size=batch_size)\n",
    "\n",
    "validation_data = DataLoader(\n",
    "              data['valid']['src'],\n",
    "              data['valid']['label'],\n",
    "              data['valid']['valid_mask'],\n",
    "              data['valid']['valid_mask_all'],\n",
    "              max_len,\n",
    "              batch_size=batch_size,\n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##############################################################################\n",
    "# Training\n",
    "# ##############################################################################\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "train_loss = []\n",
    "valid_loss = []\n",
    "accuracy = []\n",
    "\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        if use_cuda:\n",
    "            return Variable(h.data).cuda()\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "def evaluate():\n",
    "    rnn.eval()\n",
    "    corrects = eval_loss = 0\n",
    "    _size = validation_data.sents_size\n",
    "    hidden = rnn.init_hidden()\n",
    "    for data, label in tqdm(validation_data, mininterval=0.2,\n",
    "                desc='Evaluate Processing', leave=False):\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        pred, hidden = rnn(data, hidden)\n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        eval_loss += loss.data\n",
    "        corrects += (torch.max(pred, 1)[1].view(label.size()).data == label.data).sum()\n",
    "\n",
    "    return 1.0*eval_loss[0]/_size, corrects, 1.0*corrects/_size * 100.0, _size\n",
    "\n",
    "def train():\n",
    "    rnn.train()\n",
    "    total_loss = 0\n",
    "    hidden = rnn.init_hidden()\n",
    "    for data, label in tqdm(training_data, mininterval=1,\n",
    "                desc='Train Processing', leave=False):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        target, hidden = rnn(data, hidden)\n",
    "        loss = criterion(target, label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.data\n",
    "    return 1.0*total_loss[0]/training_data.sents_size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Text(\n",
      "  (lookup_table): Embedding(21427, 300, padding_idx=0)\n",
      "  (lstm): LSTM(300, 128, dropout=0.5, bidirectional=1)\n",
      "  (ln): LayerNorm(\n",
      "  )\n",
      "  (logistic): Linear(in_features=256, out_features=2, bias=True)\n",
      ")\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c0c3139c9b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-18b2a63dd711>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     for data, label in tqdm(training_data, mininterval=1,\n\u001b[0;32m---> 40\u001b[0;31m                 desc='Train Processing', leave=False):\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepackage_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import const\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        mu = torch.mean(input, dim=-1, keepdim=True)\n",
    "        sigma = torch.std(input, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "        output = (input - mu) / sigma\n",
    "        return output * self.weight.expand_as(output) + self.bias.expand_as(output)\n",
    "\n",
    "class LSTM_Text(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,batch_size,embed_dim,label_size):\n",
    "        super(LSTM_Text,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.hidden_size=128\n",
    "        self.lstm_layers=1\n",
    "        self.dropout=0.5\n",
    "        self.batch_size=batch_size\n",
    "        self.bidirectional=1\n",
    "        self.label_size=label_size\n",
    "        #self.num_directions = 2 if self.bidirectional else 1\n",
    "        self.num_directions = 2 if 1 else 1\n",
    "\n",
    "        self.lookup_table = nn.Embedding(self.vocab_size, self.embed_dim,\n",
    "                                padding_idx=const.PAD)\n",
    "        self.lstm = nn.LSTM(self.embed_dim,\n",
    "                            self.hidden_size,\n",
    "                            self.lstm_layers,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional)\n",
    "        self.ln = LayerNorm(self.hidden_size*self.num_directions)\n",
    "        self.logistic = nn.Linear(self.hidden_size*self.num_directions,\n",
    "                                self.label_size)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self, scope=1.):\n",
    "        self.lookup_table.weight.data.copy_(torch.from_numpy(pretrained_wordvec))\n",
    "        self.logistic.weight.data.uniform_(-scope, scope)\n",
    "        self.logistic.bias.data.fill_(0)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        num_layers = self.lstm_layers*self.num_directions\n",
    "\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(num_layers, self.batch_size, self.hidden_size).zero_()),Variable(weight.new(num_layers, self.batch_size, self.hidden_size).zero_()))\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #print self.lookup_table.weight.data\n",
    "        encode = self.lookup_table(input)\n",
    "        lstm_out, hidden = self.lstm(encode.transpose(0, 1), hidden)\n",
    "        output = self.ln(lstm_out)[-1]\n",
    "        return F.log_softmax(self.logistic(output)), hidden\n",
    "\n",
    "# ##############################################################################\n",
    "# Build model\n",
    "# ##############################################################################\n",
    "import model\n",
    "\n",
    "\n",
    "rnn = LSTM_Text(vocab_size=vocab_size,batch_size=batch_size,embed_dim=300,label_size=label_size)\n",
    "if use_cuda:\n",
    "    rnn = rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001, weight_decay=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print rnn\n",
    "\n",
    "\n",
    "# ##############################################################################\n",
    "# Save Model\n",
    "# ##############################################################################\n",
    "best_acc = None\n",
    "total_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    print('-' * 90)\n",
    "    for epoch in range(1, 100):\n",
    "        epoch_start_time = time.time()\n",
    "        loss = train()\n",
    "        train_loss.append(loss*1000.)\n",
    "\n",
    "        print('| start of epoch {:3d} | time: {:2.2f}s | loss {:5.6f}'.format(epoch, time.time() - epoch_start_time, loss))\n",
    "\n",
    "        loss, corrects, acc, size = evaluate()\n",
    "        valid_loss.append(loss*1000.)\n",
    "        accuracy.append(acc)\n",
    "        print acc\n",
    "        epoch_start_time = time.time()\n",
    "        print('-' * 90)\n",
    "        print('| end of epoch {:3d} | time: {:2.2f}s | loss {:.4f} | accuracy {:.4f}%({}/{})'.format(epoch, time.time() - epoch_start_time, loss, acc, corrects, size))\n",
    "        print('-' * 90)\n",
    "        if not best_acc or best_acc < corrects:\n",
    "            best_acc = corrects\n",
    "            model_state_dict = rnn.state_dict()\n",
    "            '''\n",
    "            model_source = {\n",
    "                \"settings\": args,\n",
    "                \"model\": model_state_dict,\n",
    "                \"src_dict\": data['dict']['train']\n",
    "            }\n",
    "            torch.save(model_source, args.save)\n",
    "            '''\n",
    "except KeyboardInterrupt:\n",
    "    print(\"-\"*90)\n",
    "    print(\"Exiting from training early | cost time: {:5.2f}min\".format((time.time() - total_start_time)/60.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
