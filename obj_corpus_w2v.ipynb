{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "#load pretrain vec\n",
    "model=gensim.models.KeyedVectors.load_word2vec_format('/media/kwane/3160053c-937e-4de9-a540-b28bd2802040/kwane/NLP/data/word2vec.txt',binary=False) #GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "5000\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path='./obj/'\n",
    "\n",
    "subjective_examples = list(open(path+\"quote.tok.gt9.5000\", \"r\").readlines())\n",
    "subjective_examples = [s.strip() for s in subjective_examples]\n",
    "print len(subjective_examples)\n",
    "objective_examples = list(open(path+\"plot.tok.gt9.5000\", \"r\").readlines())\n",
    "objective_examples = [s.strip() for s in objective_examples]\n",
    "print len(objective_examples)\n",
    "max_len=0\n",
    "for i,line in enumerate(subjective_examples+objective_examples):\n",
    "\n",
    "    line=str.lower(line)\n",
    "    words = line.strip().split()\n",
    "    #print words \n",
    "    if len(words)>max_len:\n",
    "        max_len=len(words)\n",
    "\n",
    "print max_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 8000 2000\n",
      "Finish dumping the data to file - [./data/obj/corpus.pt]\n",
      "words length - [23928]\n",
      "label size - [0]\n"
     ]
    }
   ],
   "source": [
    "#corpus\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from const import *\n",
    "\n",
    "word2vec=np.random.uniform(-0.25,0.25,(3,4))\n",
    "\n",
    "def word2idx(sents, word2idx):\n",
    "    return [[word2idx[w] if w in word2idx else UNK for w in s] for s in sents]\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, word2idx={}, idx_num=0):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx = idx_num\n",
    "\n",
    "    def _add(self, word):\n",
    "        if self.word2idx.get(word) is None:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx += 1\n",
    "\n",
    "    def _convert(self):\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.idx\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s(size = %d)\".format(self.__class__.__name__, len(self.idx))\n",
    "\n",
    "\n",
    "class Words(Dictionary):\n",
    "    def __init__(self):\n",
    "        word2idx = {\n",
    "            WORD[PAD]: PAD,\n",
    "            WORD[UNK]: UNK\n",
    "        }\n",
    "        super(Words,self).__init__(word2idx=word2idx, idx_num=len(word2idx))\n",
    "\n",
    "    def __call__(self, sents):\n",
    "        words = set([word for sent in sents for word in sent])\n",
    "        for word in words:\n",
    "            self._add(word)\n",
    "\n",
    "\n",
    "class Labels(Dictionary):\n",
    "    def __init__(self):\n",
    "        super(Labels,self).__init__()\n",
    "\n",
    "    def __call__(self, labels):\n",
    "        _labels = set(labels)\n",
    "        for label in _labels:\n",
    "            self._add(label)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, pos, neg, save_data, max_len):\n",
    "        self.pos = os.path.join(pos)\n",
    "        self.neg = os.path.join(neg)\n",
    "        self._save_data = save_data\n",
    "\n",
    "        self.w = Words()\n",
    "        self.l = Labels()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def parse_data(self, pos, neg):\n",
    "        \"\"\"\n",
    "        fine_grained: Whether to use the fine-grained (50-class) version of TREC\n",
    "                or the coarse grained (6-class) version.\n",
    "        \"\"\"\n",
    "        _sents, _labels = [], []\n",
    "        #pos\n",
    "        for sentence in open(pos):\n",
    "            sentence=str.lower(sentence)\n",
    "            words = sentence.strip().split()\n",
    "            \n",
    "            if len(words) > self.max_len:\n",
    "                words = words[:self.max_len]\n",
    "\n",
    "            _sents += [words]\n",
    "            _labels.append(1)\n",
    "        #neg\n",
    "        for sentence in open(neg):\n",
    "            sentence=str.lower(sentence)\n",
    "            words = sentence.strip().split()\n",
    "            \n",
    "            if len(words) > self.max_len:\n",
    "                words = words[:self.max_len]\n",
    "\n",
    "            _sents += [words]\n",
    "            _labels.append(0)\n",
    "        \n",
    "        self.w(_sents)    \n",
    "        self._sents = _sents   \n",
    "        self._labels = _labels\n",
    "    def _shuffle(self,src,_labels):\n",
    "        self.indices = np.arange(len(_labels))\n",
    "        np.random.shuffle(self.indices)\n",
    "        #print '_shuffle'\n",
    "        self.src = self.src[self.indices]\n",
    "        self._labels = self._labels[self.indices]\n",
    "\n",
    "    def save(self):\n",
    "        self.parse_data(self.pos,self.neg)\n",
    "        self.src=word2idx(self._sents, self.w.word2idx)\n",
    "        self.src=np.asarray(self.src)\n",
    "        self._labels=np.asarray(self._labels)\n",
    "        \n",
    "        self._shuffle(self.src,self._labels)\n",
    "        #print len(self.src)\n",
    "        train_size=int(len(self.src)*0.8)\n",
    "        #valid_size=len(self.src)-train_size\n",
    "        self.train_src,self.train_labels=self.src[:train_size],self._labels[:train_size]\n",
    "        self.valid_src,self.valid_labels=self.src[train_size:],self._labels[train_size:]\n",
    "        print len(self.src),len(self.train_src),len(self.valid_src)\n",
    "        def mask_(i):#mask for rnn\n",
    "            mask=np.zeros(self.max_len, dtype=int)\n",
    "            mask[i-1]=1\n",
    "            return mask\n",
    "        def mask_all(i):#mask for bi-rnn & cnn\n",
    "            mask=np.zeros(self.max_len, dtype=int)\n",
    "            mask[:i]=np.ones(i, dtype=int)\n",
    "            return mask\n",
    "        train_mask=[mask_(len(s)) for s in self.train_src]\n",
    "        valid_mask=[mask_(len(s)) for s in self.valid_src]\n",
    "        train_mask,valid_mask=np.asarray(train_mask),np.asarray(valid_mask)\n",
    "        \n",
    "        train_mask_all=[mask_all(len(s)) for s in self.train_src]\n",
    "        valid_mask_all=[mask_all(len(s)) for s in self.valid_src]\n",
    "        train_mask_all,valid_mask_all=np.asarray(train_mask_all),np.asarray(valid_mask_all)\n",
    "        #print train_mask\n",
    "        data = {\n",
    "            'max_len': self.max_len,\n",
    "            'dict': {\n",
    "                'train': self.w.word2idx,\n",
    "                'vocab_size': len(self.w),\n",
    "                'label_size': 2,\n",
    "                'indices':self.indices,\n",
    "            },\n",
    "            'train': {\n",
    "                'src': self.train_src,\n",
    "                'label': self.train_labels,\n",
    "                'train_mask':train_mask,\n",
    "                'train_mask_all':train_mask_all\n",
    "\n",
    "            },\n",
    "            'valid': {\n",
    "                'src': self.valid_src,\n",
    "                'label': self.valid_labels,\n",
    "                'valid_mask':valid_mask,\n",
    "                'valid_mask_all':valid_mask_all\n",
    "\n",
    "            }\n",
    "        }\n",
    "\n",
    "        torch.save(data, self._save_data)\n",
    "        print('Finish dumping the data to file - [{}]'.format(self._save_data))\n",
    "        print('words length - [{}]'.format(len(self.w)))\n",
    "        print('label size - [{}]'.format(len(self.l)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    corpus = Corpus(path+\"quote.tok.gt9.5000\",path+\"plot.tok.gt9.5000\", path+\"corpus.pt\", max_len)\n",
    "    corpus.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9169, 15302, 12716, 17976, 6975, 11205, 10875, 17516, 21501, 4097, 6209, 3816, 1166, 246] [7659, 5861, 12716, 7659, 20483, 12552, 8663, 9779, 11210, 7659, 12338, 21999, 21036, 17818, 13776, 6762, 248, 4700, 16422, 19737, 20269, 20055, 626, 246]\n"
     ]
    }
   ],
   "source": [
    "subjective_examples+objective_examples\n",
    "data = torch.load(path+'corpus.pt')\n",
    "vocab_size=data['dict']['vocab_size']\n",
    "train_dic_num=data['train']['src']\n",
    "valid_dic_num=data['valid']['src']\n",
    "dic_num=np.concatenate((train_dic_num,valid_dic_num))\n",
    "indices=data['dict']['indices']\n",
    "#print np.array(train_dic_num[0])[1]\n",
    "#print type(train_dic_num),type(train_dic_num[0])\n",
    "word2vec=np.zeros((vocab_size,300))\n",
    "#train\n",
    "for i,line in enumerate(subjective_examples):\n",
    "    line=str.lower(line)\n",
    "    words = line.strip().split()\n",
    "    words = words[:max_len]\n",
    "    for j,word in enumerate(words):\n",
    "        #print i,j,train_dic_num[i][j]\n",
    "        try:\n",
    "            word2vec[np.array(dic_num[int(np.where(indices==i)[0])])[j],:]=model[word]\n",
    "        except:\n",
    "            word2vec[np.array(dic_num[int(np.where(indices==i)[0])])[j],:]=np.random.uniform(-0.25,0.25,300)\n",
    "#valid\n",
    "for i,line in enumerate(objective_examples):\n",
    "    line=str.lower(line)\n",
    "    words = line.strip().split()\n",
    "    words = words[:max_len]\n",
    "    for j,word in enumerate(words):\n",
    "        try:\n",
    "            word2vec[np.array(dic_num[int(np.where(indices==len(subjective_examples)+i)[0])])[j],:]=model[word]\n",
    "        except:\n",
    "            word2vec[np.array(dic_num[int(np.where(indices==len(subjective_examples)+i)[0])])[j],:]=np.random.uniform(-0.25,0.25,300)\n",
    "print train_dic_num[0],train_dic_num[1]\n",
    "np.save(path+'pretrained_wordvec',word2vec)\n",
    "#x=np.load('./data/pretrained_wordvec.npy')\n",
    "#print np.size(word2vec),word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
