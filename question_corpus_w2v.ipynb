{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import logging\n",
    "#load pretrain vec\n",
    "\n",
    "model=gensim.models.KeyedVectors.load_word2vec_format('/media/kwane/3160053c-937e-4de9-a540-b28bd2802040/kwane/NLP/data/word2vec.txt',binary=False) #GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5452\n",
      "500\n",
      "37\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path='./question/'\n",
    "\n",
    "train = list(open(path+\"train.txt\", \"r\").readlines())\n",
    "train = [s.strip() for s in train]\n",
    "print len(train)\n",
    "valid = list(open(path+\"test.txt\", \"r\").readlines())\n",
    "valid = [s.strip() for s in valid]\n",
    "print len(valid)\n",
    "max_len=0\n",
    "for i,line in enumerate(train+valid):\n",
    "    label, _, _words = line.replace('\\xf0', ' ').partition(' ')\n",
    "    words = _words.strip().split()\n",
    "    #print words,len(words)\n",
    "    if len(words)>max_len:\n",
    "        max_len=len(words)\n",
    "\n",
    "print max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish dumping the data to file - [./data/question/corpus.pt]\n",
      "words length - [9450]\n",
      "label size - [7]\n"
     ]
    }
   ],
   "source": [
    "#corpus\n",
    "import torch\n",
    "import logging\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from const import *\n",
    "\n",
    "word2vec=np.random.uniform(-0.25,0.25,(3,4))\n",
    "\n",
    "def word2idx(sents, word2idx):\n",
    "    return [[word2idx[w] if w in word2idx else UNK for w in s] for s in sents]\n",
    "\n",
    "\n",
    "class Dictionary(object):\n",
    "    def __init__(self, word2idx={}, idx_num=0):\n",
    "        self.word2idx = word2idx\n",
    "        self.idx = idx_num\n",
    "\n",
    "    def _add(self, word):\n",
    "        if self.word2idx.get(word) is None:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx += 1\n",
    "\n",
    "    def _convert(self):\n",
    "        self.idx2word = {v: k for k, v in self.word2idx.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.idx\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s(size = %d)\".format(self.__class__.__name__, len(self.idx))\n",
    "\n",
    "\n",
    "class Words(Dictionary):\n",
    "    def __init__(self):\n",
    "        word2idx = {\n",
    "            WORD[PAD]: PAD,\n",
    "            WORD[UNK]: UNK\n",
    "        }\n",
    "        super(Words,self).__init__(word2idx=word2idx, idx_num=len(word2idx))\n",
    "\n",
    "    def __call__(self, sents):\n",
    "        words = set([word for sent in sents for word in sent])\n",
    "        for word in words:\n",
    "            self._add(word)\n",
    "\n",
    "\n",
    "class Labels(Dictionary):\n",
    "    def __init__(self):\n",
    "        super(Labels,self).__init__()\n",
    "\n",
    "    def __call__(self, labels):\n",
    "        _labels = set(labels)\n",
    "        for label in _labels:\n",
    "            self._add(label)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path, save_data, max_len):\n",
    "        self.train = os.path.join(path, \"train.txt\")\n",
    "        self.valid = os.path.join(path, \"test.txt\")\n",
    "        self._save_data = save_data\n",
    "\n",
    "        self.w = Words()\n",
    "        self.l = Labels()\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def parse_data(self, _file, is_train=True, fine_grained=False):\n",
    "        \"\"\"\n",
    "        fine_grained: Whether to use the fine-grained (50-class) version of TREC\n",
    "                or the coarse grained (6-class) version.\n",
    "        \"\"\"\n",
    "        _sents, _labels = [], []\n",
    "        for sentence in open(_file):\n",
    "            label, _, _words = sentence.replace('\\xf0', ' ').partition(' ')\n",
    "            label = label.split(\":\")[0] if not fine_grained else label\n",
    "\n",
    "            words = _words.strip().split()\n",
    "\n",
    "            if len(words) > self.max_len:\n",
    "                words = words[:self.max_len]\n",
    "\n",
    "            _sents += [words]\n",
    "            _labels += [label]\n",
    "        if is_train:\n",
    "            self.w(_sents)\n",
    "            self.l(_labels)\n",
    "            self.train_sents = _sents\n",
    "            self.train_labels = _labels\n",
    "        else:\n",
    "            self.valid_sents = _sents\n",
    "            self.valid_labels = _labels\n",
    "\n",
    "    def save(self):\n",
    "        self.parse_data(self.train)\n",
    "        self.parse_data(self.valid, False)\n",
    "        #print self.l.word2idx,[self.l.word2idx[l] for l in self.train_labels]\n",
    "        def mask_(i):#mask for rnn\n",
    "            mask=np.zeros(self.max_len, dtype=int)\n",
    "            mask[i-1]=1\n",
    "            return mask\n",
    "        def mask_all(i):#mask for bi-rnn & cnn\n",
    "            mask=np.zeros(self.max_len, dtype=int)\n",
    "            mask[:i]=np.ones(i, dtype=int)\n",
    "            return mask\n",
    "        data = {\n",
    "            'max_len': self.max_len,\n",
    "            'dict': {\n",
    "                'train': self.w.word2idx,\n",
    "                'vocab_size': len(self.w),\n",
    "                'label': self.l.word2idx,\n",
    "                'label_size': len(self.l),\n",
    "            },\n",
    "            'train': {\n",
    "                'src': word2idx(self.train_sents, self.w.word2idx),\n",
    "                'label': [self.l.word2idx[l] for l in self.train_labels],\n",
    "                'train_mask':[mask_(len(s)) for s in self.train_sents],\n",
    "                'train_mask_all':[mask_all(len(s)) for s in self.train_sents]\n",
    "            },\n",
    "            'valid': {\n",
    "                'src': word2idx(self.valid_sents, self.w.word2idx),\n",
    "                'label': [self.l.word2idx[l] for l in self.valid_labels],\n",
    "                'valid_mask':[mask_(len(s)) for s in self.valid_sents],\n",
    "                'valid_mask_all':[mask_all(len(s)) for s in self.valid_sents]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        torch.save(data, self._save_data)\n",
    "        print('Finish dumping the data to file - [{}]'.format(self._save_data))\n",
    "        print('words length - [{}]'.format(len(self.w)))\n",
    "        print('label size - [{}]'.format(len(self.l)))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \n",
    "    corpus = Corpus(path, path+\"corpus.pt\", max_len)\n",
    "    corpus.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1176, 5282, 4343, 3323, 5070, 7908, 2485, 649, 8755, 4563] [7943, 718, 4870, 2765, 5126, 7076, 1945, 4563]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = torch.load(path+'corpus.pt')\n",
    "vocab_size=data['dict']['vocab_size']\n",
    "train_dic_num=data['train']['src']\n",
    "valid_dic_num=data['valid']['src']\n",
    "\n",
    "word2vec=word2vec=np.zeros((vocab_size,300))\n",
    "#train\n",
    "for i,line in enumerate(train):\n",
    "    line=str.lower(line)\n",
    "    label, _, _words = line.replace('\\xf0', ' ').partition(' ')\n",
    "    words = _words.strip().split()\n",
    "    words = words[:max_len]\n",
    "    for j,word in enumerate(words):\n",
    "        #print i,j,train_dic_num[i][j]\n",
    "        try:\n",
    "            word2vec[train_dic_num[i][j],:]=model[word]\n",
    "        except:\n",
    "            word2vec[train_dic_num[i][j],:]=np.random.uniform(-0.25,0.25,300)\n",
    "#valid\n",
    "for i,line in enumerate(valid):\n",
    "    line=str.lower(line)\n",
    "    label, _, _words = line.replace('\\xf0', ' ').partition(' ')\n",
    "    words = _words.strip().split()\n",
    "    words = words[:max_len]\n",
    "    for j,word in enumerate(words):\n",
    "        try:\n",
    "            word2vec[valid_dic_num[i][j],:]=model[word]\n",
    "        except:\n",
    "            word2vec[valid_dic_num[i][j],:]=np.random.uniform(-0.25,0.25,300)\n",
    "print train_dic_num[0],train_dic_num[1]\n",
    "np.save(path+'pretrained_wordvec',word2vec)\n",
    "#x=np.load('./data/pretrained_wordvec.npy')\n",
    "#print np.size(word2vec),word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
