{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "torch.manual_seed(5)\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "# ############################################################################\n",
    "# param\n",
    "##############################################################################\n",
    "batch_size=50\n",
    "use_cuda=1\n",
    "##############################################################################\n",
    "# Load data\n",
    "##############################################################################\n",
    "from data_loader import DataLoader\n",
    "#question\n",
    "path='/media/kwane/3160053c-937e-4de9-a540-b28bd2802040/kwane/NLP/lstm_text_class/data/obj/'\n",
    "pretrained_wordvec=np.load(path+'pretrained_wordvec.npy')\n",
    "data = torch.load(path+'corpus.pt')\n",
    "max_len = data[\"max_len\"]\n",
    "vocab_size = data['dict']['vocab_size']\n",
    "label_size = data['dict']['label_size']\n",
    "\n",
    "training_data = DataLoader(\n",
    "             data['train']['src'],\n",
    "             data['train']['label'],\n",
    "             data['train']['train_mask'],\n",
    "             data['train']['train_mask_all'],\n",
    "             max_len,\n",
    "             batch_size=batch_size)\n",
    "\n",
    "validation_data = DataLoader(\n",
    "              data['valid']['src'],\n",
    "              data['valid']['label'],\n",
    "              data['valid']['valid_mask'],\n",
    "              data['valid']['valid_mask_all'],\n",
    "              max_len,\n",
    "              batch_size=batch_size,\n",
    "              shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##############################################################################\n",
    "# Training\n",
    "# ##############################################################################\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "def repackage_hidden(h):\n",
    "    if type(h) == Variable:\n",
    "        if use_cuda:\n",
    "            return Variable(h.data).cuda()\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "def repackage_hidden2(h):\n",
    "    if type(h) == Variable:\n",
    "        if use_cuda:\n",
    "            return Variable(h.data).cuda()\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "def train():\n",
    "    loss_plt=[]\n",
    "    hidden = rnn.init_hidden()\n",
    "    hidden2 = rnn.init_hidden2()\n",
    "    for step,(data,label,mask,mask_all) in enumerate(training_data):\n",
    "        optimizer.zero_grad()\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        hidden2 = repackage_hidden2(hidden2)\n",
    "\n",
    "        target, hidden2 = rnn(data,mask, mask_all, hidden,hidden2)\n",
    "        loss = criterion(target, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        #print step,('loss: %.3f' % ( loss.data[0]))\n",
    "        loss_plt.append(loss.data[0]) \n",
    "    return loss_plt\n",
    "\n",
    "def test():\n",
    "    rnn.eval()\n",
    "    corrects = eval_loss = 0\n",
    "    _size = validation_data.sents_size\n",
    "    hidden = rnn.init_hidden()\n",
    "    hidden2 = rnn.init_hidden2()\n",
    "    for step,(data,label,mask,mask_all) in enumerate(validation_data):\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        hidden2 = repackage_hidden2(hidden2)\n",
    "        pred, hidden2 = rnn(data,mask,mask_all, hidden,hidden2)\n",
    "        loss = criterion(pred, label)\n",
    "\n",
    "        eval_loss += loss.data\n",
    "        corrects += (torch.max(pred, 1)[1].view(label.size()).data == label.data).sum()\n",
    "\n",
    "    return 1.0*corrects/_size * 100.0,corrects,_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Text(\n",
      "  (lookup_table): Embedding(23928, 300, padding_idx=0)\n",
      "  (bi_lstm): LSTM(300, 200, dropout=0.5, bidirectional=True)\n",
      "  (lstm): LSTM(400, 400, dropout=0.5)\n",
      "  (encode_ln): LayerNorm(\n",
      "  )\n",
      "  (logistic): Linear(in_features=400, out_features=2, bias=True)\n",
      "  (softmax_word): Softmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ##############################################################################\n",
    "# Model\n",
    "# ##############################################################################import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import const\n",
    "torch.manual_seed(5)\n",
    "np.random.seed(5)\n",
    "# ## Functions to accomplish attention\n",
    "\n",
    "def batch_matmul_bias(seq, weight, bias, nonlinearity=''):\n",
    "    s = None\n",
    "    bias_dim = bias.size()\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight) \n",
    "        _s_bias = _s + bias.expand(bias_dim[0], _s.size()[0]).transpose(0,1)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s_bias = torch.tanh(_s_bias)\n",
    "        _s_bias = _s_bias.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s_bias\n",
    "        else:\n",
    "            s = torch.cat((s,_s_bias),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "def batch_matmul(seq, weight, nonlinearity=''):\n",
    "    s = None\n",
    "    for i in range(seq.size(0)):\n",
    "        _s = torch.mm(seq[i], weight)\n",
    "        if(nonlinearity=='tanh'):\n",
    "            _s = torch.tanh(_s)\n",
    "        _s = _s.unsqueeze(0)\n",
    "        if(s is None):\n",
    "            s = _s\n",
    "        else:\n",
    "            s = torch.cat((s,_s),0)\n",
    "    return s.squeeze()\n",
    "\n",
    "\n",
    "\n",
    "def attention_mul(rnn_outputs, att_weights):\n",
    "    attn_vectors = None\n",
    "    for i in range(rnn_outputs.size(0)):\n",
    "        h_i = rnn_outputs[i]\n",
    "        a_i = att_weights[i].unsqueeze(1).expand_as(h_i)\n",
    "        h_i = a_i * h_i\n",
    "        h_i = h_i.unsqueeze(0)\n",
    "        if(attn_vectors is None):\n",
    "            attn_vectors = h_i\n",
    "        else:\n",
    "            attn_vectors = torch.cat((attn_vectors,h_i),0)\n",
    "    return attn_vectors\n",
    "    #return torch.sum(attn_vectors, 0)\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, hidden_size, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.bias = nn.Parameter(torch.zeros(hidden_size))\n",
    "\n",
    "    def forward(self, input):\n",
    "        mu = torch.mean(input, dim=-1, keepdim=True)\n",
    "        sigma = torch.std(input, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "        output = (input - mu) / sigma\n",
    "        return output * self.weight.expand_as(output) + self.bias.expand_as(output)\n",
    "\n",
    "class LSTM_Text(nn.Module):\n",
    "\n",
    "    def __init__(self,vocab_size,batch_size,embed_dim,label_size):\n",
    "        super(LSTM_Text,self).__init__()\n",
    "        self.vocab_size=vocab_size\n",
    "        self.embed_dim=embed_dim\n",
    "        self.hidden_size=200\n",
    "        self.hidden_size2=400\n",
    "        self.lstm_layers=1\n",
    "        self.dropout=0.5\n",
    "        self.batch_size=batch_size\n",
    "        self.label_size=label_size\n",
    "        \n",
    "        #Bi-LSTM\n",
    "        self.bidirectional1=True#True False\n",
    "        #LSTM\n",
    "        self.bidirectional2=False\n",
    "        #RES\n",
    "        self.if_res=False\n",
    "        #Attention\n",
    "        self.if_att=True\n",
    "        self.if_att_end=False\n",
    "        \n",
    "        self.num_directions1 = 2 if self.bidirectional1 else 1\n",
    "        self.num_directions2 = 2 if self.bidirectional2 else 1\n",
    "        \n",
    "        self.lookup_table = nn.Embedding(self.vocab_size, self.embed_dim,\n",
    "                                padding_idx=const.PAD)\n",
    "        #Bi-LSTM\n",
    "        self.bi_lstm = nn.LSTM(self.embed_dim,\n",
    "                            self.hidden_size,\n",
    "                            self.lstm_layers,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional1)\n",
    "        #LSTM\n",
    "        self.lstm = nn.LSTM((self.embed_dim+self.hidden_size*self.num_directions1) if self.if_res else self.hidden_size*self.num_directions1,\n",
    "                            self.hidden_size2,\n",
    "                            self.lstm_layers,\n",
    "                            dropout=self.dropout,\n",
    "                            bidirectional=self.bidirectional2)\n",
    "        \n",
    "        #self.ln = LayerNorm(self.hidden_size*self.num_directions)\n",
    "        self.encode_ln = LayerNorm(self.embed_dim)\n",
    "\n",
    "        self.logistic = nn.Linear(self.hidden_size2*self.num_directions2,\n",
    "                                self.label_size)\n",
    "\n",
    "        self._init_weights()\n",
    "        self._init_Attention()\n",
    "    def _init_weights(self, scope=1.):\n",
    "        #self.lookup_table.weight.data.uniform_(-scope, scope)\n",
    "        self.lookup_table.weight.data.copy_(torch.from_numpy(pretrained_wordvec))\n",
    "        self.logistic.weight.data.uniform_(-scope, scope)\n",
    "        self.logistic.bias.data.fill_(0)\n",
    "        \n",
    "    def _init_Attention(self):\n",
    "        if self.if_att:\n",
    "            self.weight_W_word = nn.Parameter(torch.Tensor(self.num_directions1* self.hidden_size,self.num_directions1* self.hidden_size)).cuda()\n",
    "            self.bias_word = nn.Parameter(torch.Tensor(self.num_directions1* self.hidden_size,1)).cuda()\n",
    "            self.weight_proj_word = nn.Parameter(torch.Tensor(self.num_directions1* self.hidden_size, 1)).cuda()\n",
    "            self.softmax_word = nn.Softmax()\n",
    "            self.weight_W_word.data.uniform_(-0.1, 0.1)\n",
    "            self.weight_proj_word.data.uniform_(-0.1,0.1)\n",
    "        if self.if_att_end:\n",
    "            self.weight_W_word_end = nn.Parameter(torch.Tensor(self.num_directions2* self.hidden_size2,self.num_directions2* self.hidden_size2)).cuda()\n",
    "            self.bias_word_end = nn.Parameter(torch.Tensor(self.num_directions2* self.hidden_size2,1)).cuda()\n",
    "            self.weight_proj_word_end = nn.Parameter(torch.Tensor(self.num_directions2* self.hidden_size2, 1)).cuda()\n",
    "            self.softmax_word_end = nn.Softmax()\n",
    "            self.weight_W_word_end.data.uniform_(-0.1, 0.1)\n",
    "            self.weight_proj_word_end.data.uniform_(-0.1,0.1)\n",
    "    def init_hidden(self):\n",
    "        num_layers = self.lstm_layers*self.num_directions1\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(num_layers, self.batch_size, self.hidden_size).zero_()).cuda(),Variable(weight.new(num_layers, self.batch_size, self.hidden_size).zero_()).cuda())\n",
    "    def init_hidden2(self):\n",
    "        num_layers = self.lstm_layers*self.num_directions2\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(num_layers, self.batch_size, self.hidden_size2).zero_()).cuda(),Variable(weight.new(num_layers, self.batch_size, self.hidden_size2).zero_()).cuda())\n",
    "\n",
    "\n",
    "    def forward(self, input,mask,mask_all, hidden,hidden2):\n",
    "        #Bi-LSTM\n",
    "        encode = self.lookup_table(input)\n",
    "        encode = self.encode_ln(encode)#batch_norm\n",
    "        bilstm_out, hidden = self.bi_lstm(encode.transpose(0, 1), hidden)\n",
    "\n",
    "        #Attention\n",
    "        bilstm_out = torch.mul(bilstm_out, mask_all[:, :, None].transpose(0, 1).type(torch.cuda.FloatTensor))\n",
    "\n",
    "        if self.if_att:\n",
    "            word_squish = batch_matmul_bias(bilstm_out, self.weight_W_word,self.bias_word, nonlinearity='tanh')\n",
    "            word_attn = batch_matmul(word_squish, self.weight_proj_word)\n",
    "            word_attn_norm = self.softmax_word(word_attn.transpose(1,0))\n",
    "            word_attn_vectors = attention_mul(bilstm_out, word_attn_norm.transpose(1,0)) \n",
    "            bilstm_out=word_attn_vectors\n",
    "        #LSTM\n",
    "        if self.if_res:\n",
    "            in_lstm=torch.cat([encode.transpose(0, 1),bilstm_out],2)\n",
    "        else:\n",
    "            in_lstm=bilstm_out\n",
    "        lstm_out, hidden2 = self.lstm(in_lstm, hidden2)\n",
    "        \n",
    "        #Linear\n",
    "        output = F.tanh(lstm_out)\n",
    "        final_h = torch.mul(output, mask[:, :, None].transpose(0, 1).type(torch.cuda.FloatTensor))\n",
    "        final_h=torch.sum(final_h.transpose(0, 1), 1)\n",
    "        #print final_h.size()\n",
    "        #Attention End\n",
    "        if self.if_att_end:\n",
    "            lstm_out = torch.mul(lstm_out, mask_all[:, :, None].transpose(0, 1).type(torch.cuda.FloatTensor))\n",
    "            word_squish_end = batch_matmul_bias(lstm_out, self.weight_W_word_end,self.bias_word_end, nonlinearity='tanh')\n",
    "            word_attn_end = batch_matmul(word_squish_end, self.weight_proj_word_end)\n",
    "            word_attn_norm_end = self.softmax_word(word_attn_end.transpose(1,0))\n",
    "            word_attn_vectors_end = attention_mul(lstm_out, word_attn_norm_end.transpose(1,0)) \n",
    "            lstm_out=word_attn_vectors_end  \n",
    "            final_h = torch.mul(lstm_out, mask_all[:, :, None].transpose(0, 1).type(torch.cuda.FloatTensor))\n",
    "            final_h=torch.sum(final_h.transpose(0, 1), 1)\n",
    "\n",
    "        \n",
    "        return self.logistic(final_h), hidden2\n",
    "# ##############################################################################\n",
    "# Build model\n",
    "# ##############################################################################\n",
    "import model\n",
    "\n",
    "\n",
    "rnn = LSTM_Text(vocab_size=vocab_size,batch_size=batch_size,embed_dim=300,label_size=label_size)\n",
    "if use_cuda:\n",
    "    rnn = rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "print rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/kwane/3160053c-937e-4de9-a540-b28bd2802040/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:165: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-e051272e6ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss_plt\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-47-3e6fadf987d1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m#print step,('loss: %.3f' % ( loss.data[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss_plt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss_plt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ##############################################################################\n",
    "# Main\n",
    "# ##############################################################################\n",
    "epoches=10\n",
    "\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "loss_plt=[]\n",
    "Accuracy_=[]\n",
    "for epoch in range(epoches):\n",
    "    #train\n",
    "    loss_plt+=train()\n",
    "    \n",
    "    #test\n",
    "    Accuracy,corrects,_size=test()\n",
    "    Accuracy_.append(Accuracy)\n",
    "    print 'epoch:',epoch,'Accuracy:',str(Accuracy)+'%',str(corrects)+'/'+str(_size)#,loss_plt[-1]\n",
    "    \n",
    "plt.plot(loss_plt)  \n",
    "plt.xlabel('num')  \n",
    "plt.ylabel('loss')  \n",
    "plt.show()\n",
    "\n",
    "plt.plot(Accuracy_)  \n",
    "plt.xlabel('epochs')  \n",
    "plt.ylabel('Accuracy')  \n",
    "plt.show()\n",
    "        \n",
    "print('Finished Training')\n",
    "endtime = datetime.datetime.now()\n",
    "\n",
    "print 'starttime:',starttime,'|endtime:',endtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
